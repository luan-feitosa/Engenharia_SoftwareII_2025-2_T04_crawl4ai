{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Padr√µes Arquiteturais de Software - Engenharia de Software II"
      ],
      "metadata": {
        "id": "vdVh1dGi7XJW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5wQfU0_4-JBS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a81ce73-dab9-4a02-8659-d545d46a71fe",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (1.109.1)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.12/dist-packages (0.36.0)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.1.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.11.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai) (2.11.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai) (4.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (2.32.4)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (1.2.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.57.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.8.0+cu126)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub) (2.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n",
            "Requirement already satisfied: radon in /usr/local/lib/python3.12/dist-packages (6.0.1)\n",
            "Requirement already satisfied: mando<0.8,>=0.6 in /usr/local/lib/python3.12/dist-packages (from radon) (0.7.1)\n",
            "Requirement already satisfied: colorama>=0.4.1 in /usr/local/lib/python3.12/dist-packages (from radon) (0.4.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from mando<0.8,>=0.6->radon) (1.17.0)\n",
            "Bibliotecas prontas. (Execute !pip install ... se necess√°rio)\n"
          ]
        }
      ],
      "source": [
        "# C√©lula 1: Instala√ß√£o de Depend√™ncias\n",
        "# Descomente e execute se necess√°rio.\n",
        "# 'radon' √© para a an√°lise de depend√™ncias/complexidade em Python.\n",
        "# Troque 'radon' por 'madge' se for um projeto JS/TS.\n",
        "\n",
        "!pip install openai huggingface_hub sentence-transformers scikit-learn numpy\n",
        "!pip install radon\n",
        "\n",
        "print(\"Bibliotecas prontas. (Execute !pip install ... se necess√°rio)\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "ZXTPFJTC9HmF",
        "outputId": "066b014e-bca5-46c9-be6a-838816753291",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1408506528.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    132\u001b[0m   )\n\u001b[1;32m    133\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    135\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# C√©lula 2: Imports e Configura√ß√£o Inicial\n",
        "import os\n",
        "import subprocess\n",
        "import json\n",
        "import shutil\n",
        "from openai import OpenAI\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.cluster import KMeans\n",
        "import numpy as np\n",
        "import warnings\n",
        "from radon.cli import CCHarvester # Para an√°lise de complexidade/depend√™ncia Python\n",
        "\n",
        "# Suprimir avisos\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"sklearn\")\n",
        "\n",
        "# --- 1. CONFIGURA√á√ÉO DA API HUGGING FACE ---\n",
        "# !! IMPORTANTE !! Substitua pela sua API Key.\n",
        "HF_API_KEY = \"SUA_API_KEY_AQUI\"\n",
        "\n",
        "if HF_API_KEY == \"SUA_API_KEY_AQUI\":\n",
        "    print(\"=\"*50)\n",
        "    print(\"!! ALERTA: Por favor, insira sua API Key do Hugging Face na C√©lula 2 !!\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "client = OpenAI(\n",
        "    base_url=\"https://router.huggingface.co/v1\",\n",
        "    api_key=HF_API_KEY,\n",
        ")\n",
        "\n",
        "# --- 2. CONFIGURA√á√ÉO DO MODELO ---\n",
        "# Escolha um modelo de chat dispon√≠vel no Hugging Face\n",
        "# \"meta-llama/Llama-3.1-8B-Instruct\",\n",
        "# \"deepseek-ai/DeepSeek-V3.2-Exp\",\n",
        "# \"zai-org/GLM-4.6\",\n",
        "MODELO_HF = \"Qwen/Qwen2.5-7B-Instruct\"\n",
        "\n",
        "# --- 3. CONFIGURA√á√ÉO DO REPOSIT√ìRIO (!! IMPORTANTE !!) ---\n",
        "# !! MUDE AQUI !!\n",
        "REPO_URL = \"https://github.com/unclecode/crawl4ai.git\" # Exemplo: FastAPI (Python)\n",
        "REPO_PATH = \"repositorio_clonado\"\n",
        "\n",
        "# !! MUDE AQUI !!\n",
        "# Filtro de linguagem para o Passo 4 (Embedding)\n",
        "CODE_FILE_EXTENSION = \".py\"\n",
        "# Filtro de pastas a ignorar (otimiza a an√°lise)\n",
        "IGNORE_DIRS = {'node_modules', '.git', '.vscode', '__pycache__', 'venv', 'docs', 'tests', 'examples'}\n",
        "\n",
        "# --- 4. FUN√á√ÉO HELPER PARA CHAMAR A LLM ---\n",
        "#       Esta fun√ß√£o ir√° determinar os pap√©is da LLM e informa-la do prompt\n",
        "#       em quest√£o, depois retornar a resposta ao mesmo. Tudo isso com error handling\n",
        "def analisar_com_llm(prompt_sistema, prompt_usuario):\n",
        "    \"\"\"\n",
        "    Fun√ß√£o auxiliar para fazer chamadas √† API do Hugging Face\n",
        "    usando a interface do OpenAI.\n",
        "    \"\"\"\n",
        "    print(\"--- ü§ñ Chamando LLM para an√°lise... ---\")\n",
        "    # if HF_API_KEY == \"SUA_API_KEY_AQUI\":\n",
        "    #     print(\"!! ERRO: API Key n√£o configurada. Pulando chamada... !!\")\n",
        "    #     return \"(API Key n√£o configurada)\"\n",
        "\n",
        "    try:\n",
        "        completion = client.chat.completions.create(\n",
        "            model=MODELO_HF,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": prompt_sistema},\n",
        "                {\"role\": \"user\", \"content\": prompt_usuario},\n",
        "            ],\n",
        "            temperature=0.1,\n",
        "        )\n",
        "        resposta = completion.choices[0].message.content\n",
        "        print(\"--- ‚úÖ An√°lise da LLM recebida. ---\")\n",
        "        return resposta\n",
        "    except Exception as e:\n",
        "        print(f\"--- ‚ùå Erro ao chamar a LLM: {e} ---\")\n",
        "        return f\"(Erro na chamada da API: {e})\"\n",
        "\n",
        "print(\"Configura√ß√£o inicial conclu√≠da.\")"
      ],
      "metadata": {
        "id": "xZ7S4KQ17zWD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# C√©lula 3: Clonando o Reposit√≥rio Real\n",
        "print(\"\\n--- INICIANDO CLONE DO REPOSIT√ìRIO ---\")\n",
        "\n",
        "# Se j√° houver um reposit√≥rio nesse caminho, ele √© deletado\n",
        "if os.path.exists(REPO_PATH):\n",
        "    print(f\"Removendo diret√≥rio existente: {REPO_PATH}\")\n",
        "    shutil.rmtree(REPO_PATH)\n",
        "\n",
        "print(f\"Clonando {REPO_URL} para {REPO_PATH}...\")\n",
        "try:\n",
        "    # Usamos --depth 1 para um clone r√°pido (shallow clone)\n",
        "    # Remova --depth 1 se a an√°lise completa do Git Log (Passo 5) for crucial\n",
        "    subprocess.run(\n",
        "        [\"git\", \"clone\", REPO_URL, REPO_PATH],\n",
        "        check=True, capture_output=True, text=True\n",
        "    )\n",
        "    print(\"Reposit√≥rio clonado com sucesso.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"ERRO: 'git' n√£o encontrado. Por favor, instale o Git.\")\n",
        "except subprocess.CalledProcessError as e:\n",
        "    print(f\"ERRO ao clonar o reposit√≥rio: {e.stderr}\")\n",
        "except Exception as e:\n",
        "    print(f\"Um erro inesperado ocorreu: {e}\")"
      ],
      "metadata": {
        "id": "L2Qh7Kly-C26"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# C√©lula 4: PASSO 1 - An√°lise da Documenta√ß√£o\n",
        "print(\"\\n--- INICIANDO PASSO 1: An√°lise da Documenta√ß√£o ---\")\n",
        "\n",
        "documentacao_completa_para_chunking = \"\" # Coleta toda a documenta√ß√£o relevante para chunking posterior\n",
        "doc_files = []\n",
        "# Limita a busca para evitar arquivos .md em 'node_modules' etc.\n",
        "for root, dirs, files in os.walk(REPO_PATH):\n",
        "    # Remove diret√≥rios ignorados da busca\n",
        "    dirs[:] = [d for d in dirs if d not in IGNORE_DIRS]\n",
        "\n",
        "    for file in files:\n",
        "        if file.lower().endswith(\".md\"):\n",
        "            filepath = os.path.join(root, file)\n",
        "            doc_files.append(filepath)\n",
        "            try:\n",
        "                with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
        "                    conteudo = f.read() # L√™ o conte√∫do completo para chunking posterior\n",
        "                    # \"os.path.relpath\" retorna o caminho do conteudo, e \"conteudo\" o conteudo em si\n",
        "                    documentacao_completa_para_chunking += f\"--- CONTE√öDO DE {os.path.relpath(filepath, REPO_PATH)} ---\\n{conteudo}\\n\\n\"\n",
        "            except Exception as e:\n",
        "                print(f\"Erro ao ler {filepath}: {e}\")\n",
        "\n",
        "print(f\"Arquivos .md encontrados e lidos: {len(doc_files)}\")\n",
        "\n",
        "# --- Chunking da documenta√ß√£o para LLM ---\n",
        "# Estimativa conservadora de chars por token para o modelo (varia por modelo)\n",
        "CHARS_PER_TOKEN_ESTIMATE = 3.5\n",
        "MODEL_MAX_TOKENS = 16384\n",
        "PROMPT_OVERHEAD_TOKENS = 500 # Estimativa para o prompt do sistema + partes fixas do prompt do usu√°rio\n",
        "\n",
        "MAX_DOCUMENT_TOKENS = MODEL_MAX_TOKENS - PROMPT_OVERHEAD_TOKENS\n",
        "MAX_CHUNK_CHARS = int(MAX_DOCUMENT_TOKENS * CHARS_PER_TOKEN_ESTIMATE)\n",
        "\n",
        "documentacao_chunks = []\n",
        "if len(documentacao_completa_para_chunking) > MAX_CHUNK_CHARS:\n",
        "    for i in range(0, len(documentacao_completa_para_chunking), MAX_CHUNK_CHARS):\n",
        "        documentacao_chunks.append(documentacao_completa_para_chunking[i:i + MAX_CHUNK_CHARS])\n",
        "    print(f\"Documenta√ß√£o dividida em {len(documentacao_chunks)} chunks para an√°lise.\")\n",
        "else:\n",
        "    documentacao_chunks.append(documentacao_completa_para_chunking)\n",
        "    print(\"Documenta√ß√£o pequena o suficiente para uma √∫nica an√°lise.\")\n",
        "\n",
        "\n",
        "prompt_sistema = \"Voc√™ √© um engenheiro de software s√™nior. Sua tarefa √© analisar a documenta√ß√£o de um projeto e identificar padr√µes arquiteturais e de design.\"\n",
        "analises_parciais = []\n",
        "\n",
        "for i, chunk in enumerate(documentacao_chunks):\n",
        "    prompt_usuario_chunk = f\"\"\"\n",
        "    Com base na seguinte parte da documenta√ß√£o (parte {i+1} de {len(documentacao_chunks)}), identifique:\n",
        "    1. Quais padr√µes arquiteturais (ex: MVC, Layered, Microservi√ßos) s√£o explicitamente mencionados?\n",
        "    2. Quais princ√≠pios de design (ex: separa√ß√£o de responsabilidades, inje√ß√£o de depend√™ncia) s√£o descritos?\n",
        "    3. Fa√ßa um resumo da arquitetura proposta por esta parte da documenta√ß√£o.\n",
        "\n",
        "    --- IN√çCIO DO CHUNK {i+1} ---\n",
        "    {chunk}\n",
        "    --- FIM DO CHUNK {i+1} ---\n",
        "    \"\"\"\n",
        "    print(f\"\\n--- Analisando chunk {i+1} de {len(documentacao_chunks)} ---\")\n",
        "    analise_chunk = analisar_com_llm(prompt_sistema, prompt_usuario_chunk)\n",
        "    analises_parciais.append(f\"An√°lise do Chunk {i+1}:\\n{analise_chunk}\\n\")\n",
        "\n",
        "# --- S√≠ntese das an√°lises parciais ---\n",
        "if len(analises_parciais) > 1:\n",
        "    synthesis_prompt_sistema = \"Voc√™ √© um engenheiro de software s√™nior. Sua tarefa √© sintetizar m√∫ltiplas an√°lises parciais de documenta√ß√£o em um relat√≥rio coeso e abrangente, focado em padr√µes arquiteturais e de design.\"\n",
        "    synthesis_prompt_usuario = f\"\"\"\n",
        "    Sintetize as seguintes an√°lises parciais de documenta√ß√£o em um relat√≥rio √∫nico e coeso sobre os padr√µes arquiteturais e de design do projeto. Evite repeti√ß√µes e foque nos pontos principais de cada an√°lise. Priorize a identifica√ß√£o dos padr√µes mais relevantes e a cria√ß√£o de um resumo claro da arquitetura.\n",
        "\n",
        "    {'\\n\\n'.join(analises_parciais)}\n",
        "    \"\"\"\n",
        "    print(\"\\n--- INICIANDO S√çNTESE DAS AN√ÅLISES PARCIAIS ---\")\n",
        "\n",
        "    # Ajusta o prompt de s√≠ntese caso ele tamb√©m exceda o limite de tokens\n",
        "    if len(synthesis_prompt_usuario) / CHARS_PER_TOKEN_ESTIMATE > MODEL_MAX_TOKENS:\n",
        "        print(\"!! ALERTA: O prompt de s√≠ntese √© muito longo e ser√° truncado para caber no limite de tokens. !!\")\n",
        "        synthesis_prompt_usuario = synthesis_prompt_usuario[:int(MODEL_MAX_TOKENS * CHARS_PER_TOKEN_ESTIMATE * 0.9)] + \"\\n\\n[... O prompt de s√≠ntese foi truncado para caber no limite de tokens ...]\"\n",
        "\n",
        "\n",
        "    analise_final = analisar_com_llm(synthesis_prompt_sistema, synthesis_prompt_usuario)\n",
        "    print(\"\\n--- S√çNTESE CONCLU√çDA ---\")\n",
        "else:\n",
        "    analise_final = analises_parciais[0] if analises_parciais else \"Nenhuma documenta√ß√£o para analisar.\"\n",
        "\n",
        "# Salvando o resultado em um arquivo markdown\n",
        "output_filename = \"analise_passo1.md\"\n",
        "with open(output_filename, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(\"=== RESULTADO PASSO 1 (LLM) ===\\n\")\n",
        "    f.write(analise_final)\n",
        "    f.write(\"\\n-----------------------------------\")\n",
        "print(f\"Resultado do Passo 1 salvo em '{output_filename}'\")"
      ],
      "metadata": {
        "id": "hY2CEYfmA4gk",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat analise_passo1.md"
      ],
      "metadata": {
        "id": "VOVCqhm8ClnP",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# C√©lula 5: PASSO 2 - An√°lise da Estrutura de Pastas\n",
        "print(\"\\n--- INICIANDO PASSO 2: An√°lise da Estrutura de Pastas ---\")\n",
        "\n",
        "# Usando 'tree' (Linux/macOS) ou 'ls -R' (alternativa)\n",
        "estrutura_pastas = \"\"\n",
        "try:\n",
        "    # Tenta usar 'tree' com profundidade 3 e ignorando diret√≥rios\n",
        "    ignore_str = \"|\".join(IGNORE_DIRS)\n",
        "    comando_tree = [\"tree\", \"-L\", \"3\", \"-I\", ignore_str, REPO_PATH]\n",
        "    processo = subprocess.run(comando_tree, capture_output=True, text=True, check=True, encoding=\"utf-8\")\n",
        "    estrutura_pastas = processo.stdout\n",
        "except (FileNotFoundError, subprocess.CalledProcessError):\n",
        "    print(\"Comando 'tree' n√£o encontrado ou falhou. Gerando lista simples com pastas e arquivos.\")\n",
        "    # Alternativa mais simples: percorrer diret√≥rios e arquivos\n",
        "    lista_itens = []\n",
        "    for root, dirs, files in os.walk(REPO_PATH, topdown=True):\n",
        "        # Remove diret√≥rios ignorados da busca\n",
        "        dirs[:] = [d for d in dirs if d not in IGNORE_DIRS]\n",
        "\n",
        "        # Limita a profundidade\n",
        "        level = root.replace(REPO_PATH, '').count(os.sep)\n",
        "        if level > 3:\n",
        "            continue\n",
        "\n",
        "        indent = \" \" * 4 * level\n",
        "        lista_itens.append(f\"{indent}{os.path.basename(root)}/\")\n",
        "        for f in files:\n",
        "            lista_itens.append(f\"{indent}    {f}\") # Adiciona arquivos com indenta√ß√£o extra\n",
        "\n",
        "    estrutura_pastas = \"\\n\".join(lista_itens)\n",
        "\n",
        "print(\"\\nEstrutura de Pastas e Arquivos Gerada (N√≠vel 3):\")\n",
        "print(estrutura_pastas)\n",
        "\n",
        "# Preparando prompts para a LLM\n",
        "prompt_sistema = \"Voc√™ √© um arquiteto de software. Analise a estrutura de arquivos de um projeto para inferir seu padr√£o arquitetural.\"\n",
        "prompt_usuario = f\"\"\"\n",
        "Dada a seguinte estrutura de arquivos e pastas (N√≠vel 3):\n",
        "\n",
        "{estrutura_pastas}\n",
        "\n",
        "1. Qual padr√£o arquitetural esta estrutura sugere? (ex: Layered, baseada em features/dom√≠nio, monorepo, etc.)\n",
        "2. Justifique sua resposta com base nos nomes das pastas e arquivos.\n",
        "\"\"\"\n",
        "\n",
        "# Chamando a LLM\n",
        "analise_passo2 = analisar_com_llm(prompt_sistema, prompt_usuario)\n",
        "output_filename = \"analise_passo2.md\"\n",
        "with open(output_filename, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(\"=== RESULTADO PASSO 2 (LLM) ===\\n\")\n",
        "    f.write(analise_passo2)\n",
        "    f.write(\"\\n-----------------------------------\")\n",
        "print(f\"Resultado do Passo 2 salvo em '{output_filename}'\")"
      ],
      "metadata": {
        "id": "oRIcHU0xDkCI",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat analise_passo2.md"
      ],
      "metadata": {
        "id": "mGogEjS8EElh",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# C√©lula 6: PASSO 3 - An√°lise do Grafo de Depend√™ncias (Exemplo com Python/radon)\n",
        "print(\"\\n--- INICIANDO PASSO 3: An√°lise do Grafo de Depend√™ncias ---\")\n",
        "print(f\"Usando 'radon' para analisar c√≥digo {CODE_FILE_EXTENSION} (Python).\")\n",
        "print(\"Este passo precisa ser adaptado para outras linguagens (ex: 'madge' para JS/TS).\")\n",
        "\n",
        "# Coletar arquivos de c√≥digo\n",
        "arquivos_para_analise = []\n",
        "for root, dirs, files in os.walk(REPO_PATH):\n",
        "    dirs[:] = [d for d in dirs if d not in IGNORE_DIRS]\n",
        "    for file in files:\n",
        "        if file.endswith(CODE_FILE_EXTENSION):\n",
        "            arquivos_para_analise.append(os.path.join(root, file))\n",
        "\n",
        "grafo_dependencias = {}\n",
        "\n",
        "if CODE_FILE_EXTENSION == \".py\" and arquivos_para_analise:\n",
        "    print(f\"Analisando {len(arquivos_para_analise)} arquivos Python com 'radon'...\")\n",
        "    try:\n",
        "        # Usamos CCHarvester do radon para obter o grafo de chamadas/depend√™ncias\n",
        "        # Nota: Isso √© complexo. Uma an√°lise de 'import' seria mais simples.\n",
        "        # Vamos simplificar e focar nos 'imports'\n",
        "\n",
        "        # An√°lise de Imports (Simplificada)\n",
        "        for filepath in arquivos_para_analise[:50]: # Limita a 50 arquivos para velocidade\n",
        "            rel_path = os.path.relpath(filepath, REPO_PATH)\n",
        "            grafo_dependencias[rel_path] = []\n",
        "            with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
        "                for line in f:\n",
        "                    if line.strip().startswith(\"import \") or line.strip().startswith(\"from \"):\n",
        "                        # L√≥gica muito crua - apenas para demonstrar\n",
        "                        parts = line.split()\n",
        "                        if len(parts) > 1:\n",
        "                             # Captura 'from fastapi' ou 'import uvicorn'\n",
        "                            if parts[0] == \"from\":\n",
        "                                grafo_dependencias[rel_path].append(parts[1])\n",
        "                            elif parts[0] == \"import\":\n",
        "                                grafo_dependencias[rel_path].append(parts[1])\n",
        "\n",
        "        output_grafo_simulado = json.dumps(grafo_dependencias, indent=2)\n",
        "        print(\"Grafo de depend√™ncias (amostra de imports) gerado.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        output_grafo_simulado = f\"Erro ao analisar depend√™ncias com radon: {e}\"\n",
        "        print(output_grafo_simulado)\n",
        "else:\n",
        "    output_grafo_simulado = \"Linguagem n√£o suportada por este script ou nenhum arquivo encontrado.\"\n",
        "    print(output_grafo_simulado)\n",
        "\n",
        "\n",
        "# Preparando prompts para a LLM\n",
        "prompt_sistema = \"Voc√™ √© um analista de arquitetura de software especializado em grafos de depend√™ncia.\"\n",
        "prompt_usuario = f\"\"\"\n",
        "Analise o seguinte grafo de depend√™ncias (amostrado e simplificado, mostrando 'imports' de arquivos):\n",
        "\n",
        "{output_grafo_simulado}\n",
        "\n",
        "1. Com base nesta amostra, o projeto parece ter alta ou baixa acoplamento?\n",
        "2. Voc√™ consegue identificar depend√™ncias principais (bibliotecas externas) que definem a arquitetura? (ex: 'fastapi', 'django', 'react')\n",
        "3. Voc√™ consegue identificar 'm√≥dulos' ou 'camadas' com base nos imports internos (se houver)?\n",
        "\"\"\"\n",
        "\n",
        "# Chamando a LLM\n",
        "analise_passo3 = analisar_com_llm(prompt_sistema, prompt_usuario)\n",
        "output_filename = \"analise_passo3.md\"\n",
        "with open(output_filename, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(\"=== RESULTADO PASSO 3 (LLM) ===\\n\")\n",
        "    f.write(analise_passo3)\n",
        "    f.write(\"\\n-----------------------------------\")\n",
        "print(f\"Resultado do Passo 3 salvo em '{output_filename}'\")"
      ],
      "metadata": {
        "id": "FaW2RZTCFrB8",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat analise_passo3.md"
      ],
      "metadata": {
        "id": "VkJex6nFGSW0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# C√©lula 7: PASSO 4 - Embedding e Clusteriza√ß√£o do C√≥digo\n",
        "print(\"\\n--- INICIANDO PASSO 4: Embedding e Clusteriza√ß√£o ---\")\n",
        "\n",
        "# --- 4.1. Coletar Arquivos de C√≥digo ---\n",
        "arquivos_codigo = []\n",
        "for root, dirs, files in os.walk(REPO_PATH):\n",
        "    dirs[:] = [d for d in dirs if d not in IGNORE_DIRS]\n",
        "    for file in files:\n",
        "        if file.endswith(CODE_FILE_EXTENSION):\n",
        "            filepath = os.path.join(root, file)\n",
        "            try:\n",
        "                with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
        "                    conteudo = f.read()\n",
        "                # Usamos o caminho relativo para melhor interpreta√ß√£o\n",
        "                arquivos_codigo.append((os.path.relpath(filepath, REPO_PATH), conteudo))\n",
        "            except Exception as e:\n",
        "                print(f\"Erro ao ler {filepath}: {e}, pulando...\")\n",
        "\n",
        "print(f\"Encontrados {len(arquivos_codigo)} arquivos '{CODE_FILE_EXTENSION}' para embedding.\")\n",
        "\n",
        "# Limitar a 1000 arquivos para evitar estouro de mem√≥ria/tempo\n",
        "if len(arquivos_codigo) > 1000:\n",
        "    print(\"Reposit√≥rio muito grande. Amostrando 1000 arquivos aleatoriamente.\")\n",
        "    indices = np.random.choice(len(arquivos_codigo), 1000, replace=False)\n",
        "    arquivos_codigo = [arquivos_codigo[i] for i in indices]\n",
        "\n",
        "if not arquivos_codigo:\n",
        "    print(\"Nenhum arquivo de c√≥digo encontrado. Pulando Passo 4.\")\n",
        "    analise_passo4 = \"Nenhum arquivo de c√≥digo encontrado para clusterizar.\"\n",
        "else:\n",
        "    # --- 4.2. Gerar Embeddings ---\n",
        "    print(\"Carregando modelo de embedding (sentence-transformers)...\")\n",
        "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "    conteudos = [conteudo for _, conteudo in arquivos_codigo]\n",
        "    embeddings = model.encode(conteudos)\n",
        "    print(f\"Embeddings gerados. Shape: {embeddings.shape}\")\n",
        "\n",
        "    # --- 4.3. Clusteriza√ß√£o (K-Means) ---\n",
        "    # Definimos 6 clusters como um chute inicial razo√°vel (ex: models, controllers, services, utils, config, tests)\n",
        "    num_clusters = min(6, len(arquivos_codigo)) # N√£o ter mais clusters que arquivos\n",
        "    kmeans = KMeans(n_clusters=num_clusters, random_state=42, n_init=10)\n",
        "    labels = kmeans.fit_predict(embeddings)\n",
        "\n",
        "    clusters = {}\n",
        "    for i in range(len(arquivos_codigo)):\n",
        "        label = labels[i]\n",
        "        if label not in clusters:\n",
        "            clusters[label] = []\n",
        "        clusters[label].append(arquivos_codigo[i][0]) # Adiciona o nome do arquivo\n",
        "\n",
        "    print(\"\\nArquivos agrupados por cluster (amostra):\")\n",
        "    for label, files in clusters.items():\n",
        "        print(f\"Cluster {label}: {files[:5]}... ({len(files)} arquivos)\")\n",
        "\n",
        "    # --- 4.4. Analisar Clusters com a LLM ---\n",
        "    print(\"\\nAnalisando o 'prop√≥sito' de cada cluster com a LLM...\")\n",
        "    analise_clusters = {}\n",
        "    prompt_sistema_cluster = f\"Voc√™ √© um programador s√™nior analisando c√≥digo {CODE_FILE_EXTENSION}. Sua tarefa √© dar um r√≥tulo funcional a um cluster de arquivos (ex: 'Controladores HTTP', 'L√≥gica de Banco de Dados', 'Fun√ß√µes Utilit√°rias', 'Configura√ß√£o').\"\n",
        "\n",
        "    for label, files in clusters.items():\n",
        "        # Pegar o conte√∫do do primeiro arquivo do cluster como amostra\n",
        "        sample_filepath_rel = files[0]\n",
        "        sample_filepath_abs = os.path.join(REPO_PATH, sample_filepath_rel)\n",
        "\n",
        "        with open(sample_filepath_abs, \"r\", encoding=\"utf-8\") as f:\n",
        "            sample_content = f.read(2000) # Limita a 2000 caracteres\n",
        "\n",
        "        prompt_usuario = f\"\"\"\n",
        "        Estou analisando o **Cluster {label}**.\n",
        "        Um arquivo de amostra deste cluster √© `{sample_filepath_rel}`.\n",
        "        Seu conte√∫do (truncado) √©:\n",
        "        ```{CODE_FILE_EXTENSION}\n",
        "        {sample_content}\n",
        "        Com base nesta amostra, qual √© a prov√°vel responsabilidade principal (o r√≥tulo funcional) dos arquivos neste cluster?\n",
        "        \"\"\"\n",
        "\n",
        "        rotulo_cluster = analisar_com_llm(prompt_sistema_cluster, prompt_usuario)\n",
        "        analise_clusters[f\"Cluster {label}\"] = {\n",
        "            \"amostra_arquivos\": files[:5],\n",
        "            \"total_arquivos\": len(files),\n",
        "            \"rotulo_llm\": rotulo_cluster\n",
        "        }\n",
        "\n",
        "print(\"\\n=== RESULTADO PASSO 4 (LLM) ===\")\n",
        "analise_passo4 = json.dumps(analise_clusters, indent=2)\n",
        "output_filename = \"analise_passo4.md\"\n",
        "with open(output_filename, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(\"=== RESULTADO PASSO 4 (LLM) ===\\n\")\n",
        "    f.write(analise_passo4)\n",
        "    f.write(\"\\n-----------------------------------\")\n",
        "print(f\"Resultado do Passo 4 salvo em '{output_filename}'\")"
      ],
      "metadata": {
        "id": "BJqYnDnEGrCk",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat analise_passo4.md"
      ],
      "metadata": {
        "id": "qb3EaUmSIKaI",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# C√©lula 8: PASSO 5 - An√°lise do Git Log\n",
        "print(\"\\n--- INICIANDO PASSO 5: An√°lise do Git Log (Filtrado) ---\")\n",
        "print(\"Nota: Este passo pode falhar se voc√™ usou --depth 1 no clone.\")\n",
        "\n",
        "git_log_filtrado = \"\"\n",
        "try:\n",
        "    # Filtramos por palavras-chave arquiteturais\n",
        "    comando = [\n",
        "        \"git\", \"log\", \"--all\", \"--max-count=100\", # Limita aos √∫ltimos 100 commits\n",
        "        \"--grep=refactor\",\n",
        "        \"--grep=architecture\",\n",
        "        \"--grep=pattern\",\n",
        "        \"--grep=layer\",\n",
        "        \"-i\" # Ignora case\n",
        "    ]\n",
        "    processo = subprocess.run(comando, cwd=REPO_PATH, capture_output=True, text=True, check=True, encoding=\"utf-8\")\n",
        "    git_log_filtrado = processo.stdout\n",
        "\n",
        "    if not git_log_filtrado:\n",
        "        git_log_filtrado = \"(Nenhum commit relevante encontrado com os filtros nos √∫ltimos 100 commits)\"\n",
        "\n",
        "except subprocess.CalledProcessError as e:\n",
        "    # Se o clone foi --depth 1, o 'git log' pode n√£o ter hist√≥rico\n",
        "    if \"shallow\" in e.stderr:\n",
        "        git_log_filtrado = \"Erro: O reposit√≥rio foi clonado com '--depth 1' (shallow clone). O hist√≥rico de commits completo n√£o est√° dispon√≠vel para an√°lise. Remova '--depth 1' da C√©lula 3 para analisar o log.\"\n",
        "    else:\n",
        "        git_log_filtrado = f\"Erro ao rodar git log: {e.stderr}\"\n",
        "except Exception as e:\n",
        "    git_log_filtrado = f\"Erro ao rodar git log: {e}\"\n",
        "\n",
        "\n",
        "print(\"Log filtrado (commits com 'refactor', 'architecture', etc.):\")\n",
        "print(git_log_filtrado[:1000] + \"...\") # Mostra apenas o in√≠cio\n",
        "\n",
        "# --- Chunking do Git Log para LLM ---\n",
        "# Reutiliza as constantes de chunking da C√©lula 4\n",
        "git_log_chunks = []\n",
        "if len(git_log_filtrado) > MAX_CHUNK_CHARS:\n",
        "    for i in range(0, len(git_log_filtrado), MAX_CHUNK_CHARS):\n",
        "        git_log_chunks.append(git_log_filtrado[i:i + MAX_CHUNK_CHARS])\n",
        "    print(f\"Git Log dividido em {len(git_log_chunks)} chunks para an√°lise.\")\n",
        "else:\n",
        "    git_log_chunks.append(git_log_filtrado)\n",
        "    print(\"Git Log pequeno o suficiente para uma √∫nica an√°lise.\")\n",
        "\n",
        "\n",
        "prompt_sistema = \"Voc√™ √© um gerente de engenharia analisando o hist√≥rico de commits de um projeto para encontrar decis√µes arquiteturais.\"\n",
        "analises_parciais_git = []\n",
        "\n",
        "for i, chunk in enumerate(git_log_chunks):\n",
        "    prompt_usuario_chunk_git = f\"\"\"\n",
        "    Analise APENAS a seguinte parte do hist√≥rico de commits (parte {i+1} de {len(git_log_chunks)}).\n",
        "    O que essas mensagens de commit revelam sobre a evolu√ß√£o ou a inten√ß√£o da arquitetura do software?\n",
        "    (Se nenhuma mensagem for encontrada ou um erro for reportado, apenas declare isso).\n",
        "\n",
        "    --- IN√çCIO DO CHUNK {i+1} ---\n",
        "    {chunk}\n",
        "    --- FIM DO CHUNK {i+1} ---\n",
        "    \"\"\"\n",
        "    print(f\"\\n--- Analisando chunk {i+1} de {len(git_log_chunks)} do Git Log ---\")\n",
        "    analise_chunk_git = analisar_com_llm(prompt_sistema, prompt_usuario_chunk_git)\n",
        "    analises_parciais_git.append(f\"An√°lise do Chunk {i+1} do Git Log:\\n{analise_chunk_git}\\n\")\n",
        "\n",
        "# --- S√≠ntese das an√°lises parciais do Git Log ---\n",
        "if len(analises_parciais_git) > 1:\n",
        "    synthesis_prompt_sistema_git = \"Voc√™ √© um gerente de engenharia. Sua tarefa √© sintetizar m√∫ltiplas an√°lises parciais de hist√≥ricos de commits em um relat√≥rio coeso, focado em decis√µes e evolu√ß√£o arquitetural.\"\n",
        "    synthesis_prompt_usuario_git = f\"\"\"\n",
        "    Sintetize as seguintes an√°lises parciais do Git Log em um relat√≥rio √∫nico e coeso sobre as decis√µes e a evolu√ß√£o arquitetural do projeto. Evite repeti√ß√µes e foque nos pontos principais de cada an√°lise.\n",
        "\n",
        "    {'\\n\\n'.join(analises_parciais_git)}\n",
        "    \"\"\"\n",
        "    print(\"\\n--- INICIANDO S√çNTESE DAS AN√ÅLISES PARCIAIS DO GIT LOG ---\")\n",
        "\n",
        "    # Ajusta o prompt de s√≠ntese caso ele tamb√©m exceda o limite de tokens\n",
        "    if len(synthesis_prompt_usuario_git) / CHARS_PER_TOKEN_ESTIMATE > MODEL_MAX_TOKENS:\n",
        "        print(\"!! ALERTA: O prompt de s√≠ntese do Git Log √© muito longo e ser√° truncado para caber no limite de tokens. !!\")\n",
        "        synthesis_prompt_usuario_git = synthesis_prompt_usuario_git[:int(MODEL_MAX_TOKENS * CHARS_PER_TOKEN_ESTIMATE * 0.9)] + \"\\n\\n[... O prompt de s√≠ntese foi truncado para caber no limite de tokens ...]\"\n",
        "\n",
        "    analise_final_git = analisar_com_llm(synthesis_prompt_sistema_git, synthesis_prompt_usuario_git)\n",
        "    print(\"\\n--- S√çNTESE DO GIT LOG CONCLU√çDA ---\")\n",
        "else:\n",
        "    analise_final_git = analises_parciais_git[0] if analises_parciais_git else \"Nenhum Git Log para analisar.\"\n",
        "\n",
        "\n",
        "# Salvando o resultado em um arquivo markdown\n",
        "output_filename = \"analise_passo5.md\"\n",
        "with open(output_filename, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(\"=== RESULTADO PASSO 5 (LLM) ===\\n\")\n",
        "    f.write(analise_final_git)\n",
        "    f.write(\"\\n-----------------------------------\")\n",
        "print(f\"Resultado do Passo 5 salvo em '{output_filename}'\")"
      ],
      "metadata": {
        "id": "j4f6OVtGI8OW",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat analise_passo5.md"
      ],
      "metadata": {
        "id": "2rv02kLrKAkN",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# C√©lula 9: PASSO 6 - An√°lise Final (S√≠ntese)\n",
        "print(\"\\n--- INICIANDO PASSO 6: S√≠ntese Final ---\")\n",
        "print(\"Combinando todas as an√°lises para o relat√≥rio final...\")\n",
        "\n",
        "# Preparando o prompt final com todas as evid√™ncias\n",
        "prompt_sistema_final = \"\"\"\n",
        "Voc√™ √© um Arquiteto de Software S√™nior e est√° escrevendo o relat√≥rio final de uma an√°lise de reposit√≥rio.\n",
        "Voc√™ recebeu 5 blocos de evid√™ncias. Sua tarefa √© sintetizar TODAS elas em uma conclus√£o coesa e justificada.\n",
        "Seja direto e use as evid√™ncias para apoiar sua conclus√£o.\n",
        "\"\"\"\n",
        "\n",
        "prompt_usuario_final = f\"\"\"\n",
        "Por favor, gere um relat√≥rio de an√°lise arquitetural final.\n",
        "Combine as seguintes 5 fontes de evid√™ncia para chegar a uma conclus√£o sobre os padr√µes arquiteturais de engenharia de software deste projeto.\n",
        "\n",
        "--- EVID√äNCIA 1: An√°lise da Documenta√ß√£o ---\n",
        "{analise_passo1}\n",
        "\n",
        "--- EVID√äNCIA 2: An√°lise da Estrutura de Pastas ---\n",
        "{analise_passo2}\n",
        "\n",
        "--- EVID√äNCIA 3: An√°lise do Grafo de Depend√™ncias ---\n",
        "{analise_passo3}\n",
        "\n",
        "--- EVID√äNCIA 4: An√°lise da Clusteriza√ß√£o de C√≥digo ---\n",
        "{analise_passo4}\n",
        "\n",
        "--- EVID√äNCIA 5: An√°lise do Git Log ---\n",
        "{analise_passo5}\n",
        "\n",
        "--- RELAT√ìRIO FINAL ---\n",
        "Com base em TUDO acima, responda:\n",
        "1. Qual √© o padr√£o arquitetural prim√°rio identificado?\n",
        "2. As evid√™ncias s√£o consistentes ou contradit√≥rias? (Ex: A documenta√ß√£o bate com o c√≥digo? A estrutura de pastas reflete os clusters?)\n",
        "3. Forne√ßa uma justificativa resumida para sua conclus√£o, citando pelo menos 3 das fontes de evid√™ncia.\n",
        "\"\"\"\n",
        "\n",
        "# Chamando a LLM para o relat√≥rio final\n",
        "print(\"\\nGerando relat√≥rio final...\")\n",
        "relatorio_final = analisar_com_llm(prompt_sistema_final, prompt_usuario_final)\n",
        "output_filename = \"analise_passo6.md\"\n",
        "with open(output_filename, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(\"=== RESULTADO PASSO 6 (LLM) ===\\n\")\n",
        "    f.write(relatorio_final)\n",
        "    f.write(\"\\n-----------------------------------\")\n",
        "print(f\"Resultado do Passo 6 salvo em '{output_filename}'\")"
      ],
      "metadata": {
        "id": "iXW9KpTZKPKX",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat analise_passo6.md"
      ],
      "metadata": {
        "id": "9wamnhgWKqMj",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}